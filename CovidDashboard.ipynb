{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54904bd8-5b9f-40e3-bb50-35f1975f72d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "csv_folder_path = \"./csse_covid_19_data/csse_covid_19_daily_reports\"\n",
    "abs_csv_folder_path = os.path.abspath(csv_folder_path);\n",
    "print(\"Absolute Path : \", abs_csv_folder_path);\n",
    "\n",
    "csv_files = [os.path.join(abs_csv_folder_path, f) for f in os.listdir(abs_csv_folder_path) if f.endswith(\".csv\")]\n",
    "print(f\"Number of files to be processed :{len(csv_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3298d93b-a8da-44ae-8c09-afa4013ce3c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import concurrent.futures\n",
    "\n",
    "def read_first_line(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            return (file_path, file.readline().strip())  # Read first line and remove newline\n",
    "    except Exception as e:\n",
    "        return (file_path, f\"Error: {e}\")\n",
    "    \n",
    "first_lines = []\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    results = executor.map(read_first_line, csv_files)\n",
    "    first_lines = list(results)\n",
    "\n",
    "# Keep a mapping of header name to list of files that have the same header, this is to because then we can load all the files using the spark, rather than loading then one by one. which takes a long time. \n",
    "# First time I tried loading the files one by one around 1143, It took more than 1 hour, but still didnt complete the jobs. ie. first read file, load it to a df, and then append it a root df. and once all files are loaded, save that to a table. \n",
    "header_to_file = {}\n",
    "\n",
    "for (file_path, first_line) in first_lines:\n",
    "    if first_line not in header_to_file:\n",
    "        header_to_file[first_line] = [];\n",
    "    header_to_file[first_line].append(f\"file:{file_path}\");\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eec9507-9939-448b-957b-6c3109badc71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE DATABASE IF NOT EXISTS covid_stat;\n",
    "USE  covid_stat;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bec411c9-8ca2-48e3-9ae6-054987cd9db7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import input_file_name, monotonically_increasing_id, split, element_at, concat_ws\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    TimestampType,\n",
    "    DecimalType,\n",
    "    DoubleType,\n",
    "    LongType,\n",
    ")\n",
    "import os\n",
    "\n",
    "confirmation = input(\"‚ö†Ô∏è This action is irreversible. Type 'YES' to proceed: \")\n",
    "\n",
    "if confirmation.upper() != \"YES\":\n",
    "    raise Exception(\"üö´ Execution aborted!\")\n",
    "\n",
    "# CSV files structure, and whats in side of them, The issue here is the column names and the column index are no consistent. so there could be an issue if load all the files as it is. So we keep a name mapping of different column names and put that in to a schema.\n",
    "\n",
    "# Following are the column headings found in the CSV files.\n",
    "# FIPS,Admin2,Province_State,Country_Region,Last_Update,Lat,Long_,Confirmed,Deaths,Recovered,Active,Combined_Key,Incident_Rate,Case_Fatality_Ratio\n",
    "# FIPS,Admin2,Province_State,Country_Region,Last_Update,Lat,Long_,Confirmed,Deaths,Recovered,Active,Combined_Key\n",
    "## All the unique columes names in all files\n",
    "#{'Admin2', 'Country/Region', 'Province/State', 'Last Update', 'Deaths', 'Longitude', 'Long_', 'Lat', 'Combined_Key', 'Case_Fatality_Ratio', 'FIPS', 'Confirmed', 'Incidence_Rate', 'Province_State', 'Active', 'Latitude', 'Incident_Rate', 'Recovered', 'Last_Update', 'Country_Region', 'Case-Fatality_Ratio'}\n",
    "\n",
    "\n",
    "# FIPS: US only. Federal Information Processing Standards code that uniquely identifies counties within the USA.\n",
    "# Admin2: County name. US only.\n",
    "# Province_State: Province, state or dependency name.\n",
    "# Country_Region: Country, region or sovereignty name. The names of locations included on the Website correspond with the official designations used by the U.S. Department of State.\n",
    "# Last Update: MM/DD/YYYY HH:mm:ss (24 hour format, in UTC).\n",
    "# Lat and Long_: Dot locations on the dashboard. All points (except for Australia) shown on the map are based on geographic centroids, and are not representative of a specific address, building or any location at a spatial scale finer than a province/state. Australian dots are located at the centroid of the largest city in each state.\n",
    "# Confirmed: Counts include confirmed and probable (where reported).\n",
    "# Deaths: Counts include confirmed and probable (where reported).\n",
    "# Recovered: Recovered cases are estimates based on local media reports, and state and local reporting when available, and therefore may be substantially lower than the true number. US state-level recovered cases are from COVID Tracking Project. We stopped to maintain the recovered cases (see Issue #3464 and Issue #4465).\n",
    "# Active: Active cases = total cases - total recovered - total deaths. This value is for reference only after we stopped to report the recovered cases (see Issue #4465)\n",
    "# Incident_Rate: Incidence Rate = cases per 100,000 persons.\n",
    "# Case_Fatality_Ratio (%): Case-Fatality Ratio (%) = Number recorded deaths / Number cases.\n",
    "# All cases, deaths, and recoveries reported are based on the date of initial report. Exceptions to this are noted in the \"Data Modification\" and \"Retrospective reporting of (probable) cases and deaths\" subsections below.\n",
    "\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"FIPS\", StringType(), True),\n",
    "        StructField(\"AdminFlag\", StringType(), True),\n",
    "        StructField(\"StateOrProvince\", StringType(), True),\n",
    "        StructField(\"Country\", StringType(), True),\n",
    "        StructField(\"LastUpdated\", TimestampType(), True),\n",
    "        StructField(\"Lat\", DecimalType(9, 6), True),\n",
    "        StructField(\"Long_\", DecimalType(9, 6), True),\n",
    "        StructField(\"Confirmed\", LongType(), True),\n",
    "        StructField(\"Deaths\", LongType(), True),\n",
    "        StructField(\"Recovered\", LongType(), True),\n",
    "        StructField(\"Active\", LongType(), True),\n",
    "        StructField(\"IncidentRate\", DoubleType(), True),\n",
    "        StructField(\"CaseFatalityRatio\", DoubleType(), True),\n",
    "        StructField(\"Source\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "column_mapping = {\n",
    "    \"FIPS\": \"FIPS\",\n",
    "    \"Admin2\": \"AdminFlag\",\n",
    "    \"Province_State\": \"StateOrProvince\",\n",
    "    \"Province/State\": \"StateOrProvince\",\n",
    "    \"Country_Region\": \"Country\",\n",
    "    \"Country/Region\": \"Country\",\n",
    "    \"Last_Update\": \"LastUpdated\",\n",
    "    \"Last Update\": \"LastUpdated\",\n",
    "    \"Lat\": \"Lat\",\n",
    "    \"Latitude\" : \"Lat\",\n",
    "    \"Long_\": \"Long_\",\n",
    "    \"Longitude\" : \"Long_\",\n",
    "    \"Confirmed\": \"Confirmed\",\n",
    "    \"Deaths\": \"Deaths\",\n",
    "    \"Recovered\": \"Recovered\",\n",
    "    \"Active\": \"Active\",\n",
    "    \"Incident_Rate\": \"IncidentRate\",\n",
    "    \"Incidence_Rate\": \"IncidentRate\",\n",
    "    \"Case_Fatality_Ratio\": \"CaseFatalityRatio\",\n",
    "    \"Case-Fatality_Ratio\": \"CaseFatalityRatio\",\n",
    "    \"Source\": \"Source\",\n",
    "}\n",
    "\n",
    "\n",
    "# Set preferred Parquet file size (~256MB)\n",
    "spark.conf.set(\"parquet.block.size\", 256 * 1024 * 1024)\n",
    "# Ensure partitions generate reasonable file sizes\n",
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"256MB\")\n",
    "\n",
    "root_df = spark.createDataFrame([], schema)\n",
    "\n",
    "for key in header_to_file:\n",
    "    df = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"pathGlobFilter\", \"*.csv\")\\\n",
    "        .load(header_to_file[key])\\\n",
    "        .withColumn(\"Source\", concat_ws (\":\",input_file_name(),monotonically_increasing_id()))\\\n",
    "        .drop(\"Combined_Key\")\n",
    "\n",
    "    new_columns = [column_mapping.get(col, col) for col in df.columns]\n",
    "    df = df.toDF(*new_columns)\n",
    "    root_df = root_df.unionByName(df, allowMissingColumns=True)\n",
    "\n",
    "# # Convert to Delta for better performance\n",
    "root_df.write.format(\"delta\")\\\n",
    "      .mode(\"overwrite\")\\\n",
    "      .option(\"overwriteSchema\", \"true\")\\\n",
    "      .partitionBy(\"Country\")\\\n",
    "      .saveAsTable(\"daily_reports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb0fcead-ab28-4d47-8721-d620c25b59f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- DESCRIBE TABLE EXTENDED daily_reports\n",
    "-- DESCRIBE DETAIL daily_reports\n",
    "DESCRIBE DETAIL daily_reports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d25cd94e-48ac-4a09-b878-288518f41d94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- SELECT COUNT(*) from daily_reports\n",
    "SELECT * FROM daily_reports limit 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3721fee-25ca-466a-9cee-d2fa10ea1524",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "confirmation = input(\"‚ö†Ô∏è This action is irreversible. Type 'YES' to proceed: \")\n",
    "\n",
    "if confirmation.upper() != \"YES\":\n",
    "    raise Exception(\"üö´ Execution aborted!\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8680562668562881,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "CovidDashboard",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
